# ğŸ® Street Fighter III AI Agent â€“ DIAMBRA / 0G Cambridge Hackathon

An end-to-end Deep Reinforcement Learning project where I trained an AI agent to play **Street Fighter III: 3rd Strike** using **DIAMBRA Arena + PPO**, developed for the **DIAMBRA / 0G Cambridge University AI Hackathon (Nov 2025).**

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Status](https://img.shields.io/badge/Agent%202-Training%20In%20Progress-yellow.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

---

# ğŸ“– Project Overview

This repository documents my **complete learning journey** from building **Agent 1** (slow, inefficient, poorly structured) to **Agent 2**, which adopts best practices from DIAMBRA documentation and RL research.

The goal:
ğŸ‘‰ Train a competitive agent that can fight in Street Fighter III using optimized PPO, parallel environments, RAM state features, temporal memory, and better hyperparameters.

---

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚          Street Fighter III           â”‚
                         â”‚    (DIAMBRA Engine - C++ backend)     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â–²
                                        â”‚ Frames (128Ã—128Ã—1)
                                        â”‚ RAM States (health, timer, side,â€¦)
                                        â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DIAMBRA Arena + WrappersSettings                      â”‚
â”‚                                                                            â”‚
â”‚  â€¢ Engine-level grayscale frame resize (128Ã—128Ã—1)                         â”‚
â”‚  â€¢ Frame stacking (4)                                                      â”‚
â”‚  â€¢ Action history stacking (12)                                            â”‚
â”‚  â€¢ Reward normalization                                                     â”‚
â”‚  â€¢ RAM state extraction (8 keys)                                           â”‚
â”‚  â€¢ Filtering â†’ {'frame_stack', 'ram_state', 'last_actions'}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Stable-Baselines3 Vector Environments                    â”‚
â”‚                                                                            â”‚
â”‚  diambra run -s=6                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚ Env #1        â”‚   â”‚ Env #2        â”‚   â”‚ Env #3        â”‚   â”‚ Env #6        â”‚
â”‚  â”‚ (Docker)      â”‚   â”‚ (Docker)      â”‚   â”‚ (Docker)      â”‚   â”‚ (Docker)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                                                            â”‚
â”‚  â€¢ Parallel rollout generation (6 envs)                                    â”‚
â”‚  â€¢ ~15 FPS combined                                                         â”‚
â”‚  â€¢ Faster, more diverse experience                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MultiInputPolicy Neural Network (SB3 PPO)                 â”‚
â”‚                                                                            â”‚
â”‚  Inputs:                                                                   â”‚
â”‚   â€¢ Frame stack (4 Ã— 128 Ã— 128 grayscale)                                  â”‚
â”‚   â€¢ RAM features (health, positions, timer, character, stageâ€¦)             â”‚
â”‚   â€¢ Action history (12 last actions)                                       â”‚
â”‚                                                                            â”‚
â”‚  Architecture:                                                             â”‚
â”‚   â€¢ CNN encoder for vision                                                 â”‚
â”‚   â€¢ MLP for RAM + action memory                                            â”‚
â”‚   â€¢ Combined latent vector                                                 â”‚
â”‚   â€¢ Policy head (actions)                                                  â”‚
â”‚   â€¢ Value head (state value)                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PPO Training Loop (Optimized)                        â”‚
â”‚                                                                            â”‚
â”‚   â€¢ n_steps = 128 (small rollouts)                                         â”‚
â”‚   â€¢ batch_size = 256                                                       â”‚
â”‚   â€¢ n_epochs = 4                                                           â”‚
â”‚   â€¢ gamma = 0.94                                                           â”‚
â”‚   â€¢ Learning rate: 2.5e-4 â†’ 2.5e-6 (linear)                                â”‚
â”‚   â€¢ Clip range: 0.15 â†’ 0.025 (linear)                                      â”‚
â”‚   â€¢ Monitor: clip_fraction, approx_kl, explained_variance                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            Checkpoints + Logs                              â”‚
â”‚                                                                            â”‚
â”‚   ./checkpoints_agent2/     â†’ Auto-saved PPO checkpoints                    â”‚
â”‚   ./logs_agent2/            â†’ TensorBoard metrics                           â”‚
â”‚                                                                            â”‚
â”‚   Track:                                                         â”‚
â”‚    â€¢ ep_rew_mean             â€¢ loss                                     â”‚
â”‚    â€¢ fps                     â€¢ explained_variance                        â”‚
â”‚    â€¢ clip_fraction           â€¢ policy_entropy                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            Final Agent Export                              â”‚
â”‚                                                                            â”‚
â”‚      agent2_final.zip  â†’ Ready for DIAMBRA / 0G Hackathon submission      â”‚
â”‚                                                                            â”‚
â”‚   Run with:                                                                â”‚
â”‚      diambra run python agent.py                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


# ğŸ”´ Agent 1 â€” What Went Wrong

My first agent was functional but deeply inefficient.
Here are the mistakes and what I learned:

| Mistake             | What I Did                | What I Should Have Done                    |
| ------------------- | ------------------------- | ------------------------------------------ |
| **Wrappers**        | Built custom Gym wrappers | Use DIAMBRAâ€™s optimized `WrappersSettings` |
| **Frame Resize**    | Python resize (slow)      | Engine-level resize (fast, C++)            |
| **Color Mode**      | RGB (3 channels)          | Grayscale (1 channel, 3Ã— lighter)          |
| **Policy**          | `CnnPolicy`               | `MultiInputPolicy` (pixels + RAM)          |
| **Action Memory**   | âŒ None                    | 12-step action history (combos)            |
| **RAM Features**    | âŒ Ignored                 | Health, side, timer, stage, character      |
| **LR & Clip Range** | Static                    | Linear decay                               |
| **Gamma**           | 0.99                      | 0.94 (faster environments)                 |
| **Rollout Length**  | 2048                      | 128 (more responsive)                      |
| **Environments**    | Only 1                    | Multiple parallel envs                     |
| **FPS**             | 8                         | Target 15+                                 |

### Agent 1 Final Stats

```
Training Time: 31 hours
Total Timesteps: 1,001,472
FPS: 8
explained_variance: 0.905
clip_fraction: 0.51 (too high â†’ unstable updates)
approx_kl: 0.039
```

Agent 1 trained, but inefficiently.
It became the baseline for the improved version.

---

# ğŸŸ¢ Agent 2 â€” The Major Improvements

Agent 2 implements **best practices** found in DIAMBRA docs and fighting-game RL research.

### âœ” 1. Native DIAMBRA Wrappers

```python
from diambra.arena import WrappersSettings
wrappers_settings = WrappersSettings()
wrappers_settings.stack_frames = 4
wrappers_settings.add_last_action = True
wrappers_settings.stack_actions = 12
wrappers_settings.normalize_reward = True
```

### âœ” 2. Engine-level Frame Processing

```python
settings = EnvironmentSettings()
settings.frame_shape = (128, 128, 1)  # Grayscale
```

### âœ” 3. MultiInputPolicy (Pixels + RAM)

```python
model = PPO("MultiInputPolicy", env, ...)
wrappers_settings.filter_keys = [
    "action", "own_health", "opp_health",
    "own_side", "opp_side",
    "opp_character", "stage", "timer"
]
```

### âœ” 4. Linear Schedules

```python
def linear_schedule(start, end):
    return lambda p: end + p * (start - end)

learning_rate = linear_schedule(2.5e-4, 2.5e-6)
clip_range = linear_schedule(0.15, 0.025)
```

### âœ” 5. Fighting-Game PPO Hyperparameters

```python
gamma = 0.94
n_steps = 128
batch_size = 256
n_epochs = 4
```

### âœ” 6. Parallel Environments (Huge Speedup)

```
diambra run -s=6 python train_agent_v2_fast.py
```

---

# ğŸ“Š Agent 1 vs Agent 2

| Feature          | Agent 1             | Agent 2           |
| ---------------- | ------------------- | ----------------- |
| Frame Processing | Python, RGB         | Engine, Grayscale |
| Policy           | CnnPolicy           | MultiInputPolicy  |
| Frame Stack      | 4 via VecFrameStack | Native stack      |
| Action History   | âŒ None              | âœ” 12 actions      |
| RAM Features     | âŒ None              | âœ” 8 keys          |
| Reward Norm      | Custom              | Built-in          |
| LR Schedule      | Static              | Linear decay      |
| Gamma            | 0.99                | 0.94              |
| n_steps          | 2048                | 128               |
| Envs             | 1                   | 6                 |
| FPS              | 8                   | ~15               |
| Clip Fraction    | 0.51                | ~0.01             |
| approx_kl        | 0.039               | 0.001â€“0.002       |

Agent 2 learns **faster**, **more stably**, and **more efficiently**.

---

# ğŸ§  Key Lessons Learned

1. **Use DIAMBRAâ€™s wrappers** â€” they are optimized for exactly this purpose.
2. **RAM states dramatically improve behavior**.
3. **Action memory is essential for combos**.
4. **Fighting games â‰  Atari** â†’ different hyperparameters.
5. **Engine-side resizing is ~10Ã— faster** than Python.
6. **Parallel training is the biggest speed boost**.
7. **Monitor clip_fraction** â†’ >0.2 means bad updates.
8. **Documentation matters** â€” DIAMBRA provides great defaults.

---

# ğŸ›  Installation

```bash
git clone https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
cd YOUR_REPO_NAME

python -m venv venv
source venv/bin/activate

pip install diambra-arena[stable-baselines3]
pip install torch tensorboard

export DIAMBRAROMSPATH=/path/to/roms/
```

---

# ğŸš€ Usage

### Train Agent 2

```bash
diambra run -s=6 python train_agent_v2_fast.py
```

### Monitor Training

```
tensorboard --logdir=logs_agent2
```

### Run Trained Agent

```bash
diambra run python agent.py
```

---

# ğŸ“ Project Structure

```
â”œâ”€â”€ train_agent_v2_fast.py     # Optimized Agent 2 training
â”œâ”€â”€ agent.py                   # Inference script
â”œâ”€â”€ models_agent2/
â”‚   â””â”€â”€ agent2_final.zip
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ppo_sfiii3_final.zip   # Agent 1 baseline
â”œâ”€â”€ checkpoints_agent2/
â”œâ”€â”€ logs_agent2/
â””â”€â”€ README.md
```

---

# ğŸ“„ Submission Instructions (0G)

1. Train your model.
2. Upload model folder:

   ```bash
   0g-storage upload models_agent2/
   ```
3. Submit the returned CID to the hackathon portal.

---

# ğŸ† Hackathon Details

* Event: **DIAMBRA / 0G Cambridge AI Hackathon**
* Challenge: Train RL agents for Street Fighter III
* Submission: Via 0G Storage
* Finals: Live AI-vs-AI tournament

---

# ğŸ™ Acknowledgments

* **DIAMBRA Team** â€” for the RL fighting-game platform
* **0G Labs** â€” for decentralized model storage
* **Cambridge University** â€” for hosting the event
* **PyTorch / SB3 communities** â€” invaluable tools

---

# ğŸ“„ License

MIT License

---
